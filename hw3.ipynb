{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b0a12a",
   "metadata": {},
   "source": [
    "# CS 375 Homework 3: Logistic Regression \n",
    "\n",
    "The goal of this assignment is to give you practice implementing a `Logistic Regression` classifier and evaluate the classifier's performance on real-world datasets. \n",
    "\n",
    "In this assignment, you can assume the class labels (y) are **binary**. \n",
    "\n",
    "*Optional but recommended:* We will use `numpy` for this assignment. If you have not used numpy before or would like a refresher, please review `numpy_tutorial.ipynb` in this repository before beginning the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccdcf0c",
   "metadata": {},
   "source": [
    "## Organization and Instructions \n",
    "\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins. \n",
    "\n",
    "**Part 1: Background.**\n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "\n",
    "**Part 2: Your implementation.** This is where you will implement your solution by modifying the following:\n",
    "- `logistic_function()`\n",
    "- `NLLLoss()`\n",
    "- `gradient_nll()` \n",
    "- Implementing the `LogisticRegressionClassifier` class and its methods: \n",
    "    - `__init__()`\n",
    "    - `train()`\n",
    "    - `predict()` \n",
    "    - `get_weights()` \n",
    "\n",
    "**Part 3: Evaluation on real datasets.**\n",
    "- 3A. You will train and evaluate on the `triage` data. \n",
    "- 3B. You will examine the weights of your trained classifer.\n",
    "- 3C. You will contemplate tradeoffs between Logistic Regression and Naive Bayes classifiers and write a response.  \n",
    "\n",
    "**(Optional) Part 4: Extra Credit.** Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission. \n",
    "\n",
    "**Addtional instructions.** \n",
    "- Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "- However, you can talk to classmates about *high-level* approaches. In the **Process Reporting** section, record the names of any classmates you spoke with about this assignment. \n",
    "\n",
    "**Evaluation.**\n",
    "\n",
    "Your solution will be evaluated on accuracy on the train, dev and test sets, similar to HW2. Our reference implemenation obtained the following accuracies on the `triage` dataset: \n",
    "- `dev`: 0.763\n",
    "- `test`: 0.782\n",
    "\n",
    "However, we're going to be a bit more generous for this assignment and place `target minimum accuracies` as \n",
    "- `dev`: 0.7\n",
    "- `test`: 0.7 \n",
    "\n",
    "For logistic regression, we're not grading the training set accuracy because it could vary depending on randomness from stochastic gradient descent. \n",
    "\n",
    "**Grading.**\n",
    "\n",
    "- **10 points (autograded):** `logistic_fucntion()` unit tests. \n",
    "\n",
    "- **10 points (autograded):** `NLLLoss()` unit tests.\n",
    "\n",
    "\n",
    "- **10 points (autograded):** `gradient_nll()` unit tests.\n",
    "    \n",
    "- **10 points (autograded):** This portion of your grade reflects how well your submission performs on the `dev set` of the `triage` dataset compared to our reference implementation metrics.  Your points are calculated as \n",
    "    ```\n",
    "    (1 -(0.7 - min(accuracy on dev, 0.7))/0.7) * 10 points \n",
    "    ```\n",
    "- **10 points (autograded):** This portion of your grade reflects how well your submission performs on the `test set` of the `triage` dataset compared to our reference implementation metrics. You will not have access to the test set but will be able to see your score on Gradescope. Your points are calculated as   \n",
    "    ```\n",
    "    (1-( 0.7 - min(accuracy on test,  0.7))/ 0.7) * 10 points \n",
    "    ```  \n",
    "    \n",
    "- **5 points (manually graded):** The TAs and instructor will manually grade your answer to Part 3C. \n",
    "\n",
    "**Submission.** \n",
    "Once you have completed Parts 1, 2 and 3, run the final cell in this notebook. This will create `submission.zip` which you will then upload to Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6bad0",
   "metadata": {},
   "source": [
    "## 1A. Environment set-up\n",
    "\n",
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2d631",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for this assignment \n",
    "# Do not modify this cell \n",
    "from collections import defaultdict, Counter\n",
    "import operator\n",
    "import random\n",
    "from typing import List, Dict, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util import * #helper functions for this assignment located in util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2f06e",
   "metadata": {},
   "source": [
    "**Note:** In this assignment, you are **NOT** allowed to import or use any other packages outside the Python standard and the ones we imported above.\n",
    "\n",
    "This means you should not use any other functions from `spaCy`, `NLTK`, `gensim`, or `scikit-learn`, even though those are provided in the conda environment we set up for you. If your solution uses any such extra dependencies it will fail the autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for numpy \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c5ae8",
   "metadata": {},
   "source": [
    "## 1B. Data exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e43b90",
   "metadata": {},
   "source": [
    "In this assignment, we will be using the `triage` dataset, which we also used for HW2. Feel free to return to HW2 for a longer description of this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33614404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this data via the function from util.py\n",
    "triage_dataset = load_data(\"./data/triage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run this cell several times to look at different examples \n",
    "random_index = random.randint(0, len(triage_dataset.train))\n",
    "print(f\"Training example {random_index}:\")\n",
    "print(f\"Label(y) = {triage_dataset.train[random_index].label}\")\n",
    "tokens = triage_dataset.train[random_index].words\n",
    "text = \" \".join(tokens)\n",
    "print(f\"Text = {text}\")\n",
    "print()\n",
    "print(f\"Tokens = {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392fe7cf",
   "metadata": {},
   "source": [
    "Unlike HW2, for logistic regression we will represent the input documents as a matrix (specifically a 2-D np.ndarray) for which rows are documents and columns are counts of words. Inspect the `transform_examples_to_arrays()` function in `util.py` to see how we use `CountVectorizer` from `sklearn` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e68646",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transform_examples_to_arrays(triage_dataset.train, triage_dataset.dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631af31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words in our training vocabulary\n",
    "data['x_columns_as_words'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ten words from out training vocabulary \n",
    "data['x_columns_as_words'][100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training X is ~21K documents (rows)\n",
    "# With counts for each word in our vocabulary (columns)\n",
    "data['X_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712808d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Y labels \n",
    "data['Y_train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78dc36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 10 labels in our training dataset \n",
    "data['Y_train'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df41af0",
   "metadata": {},
   "source": [
    "## 2. Your solution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12346b",
   "metadata": {},
   "source": [
    "### 2A. Logistic function \n",
    "\n",
    "One of the building blocks of `LogisticRegression` is the `Logistic` function which we described during lecture. Implement the logistic function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a938bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_function(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Impelmentation of the logistic function \n",
    "\n",
    "    Args:\n",
    "        z: A numpy array.\n",
    "    Returns:\n",
    "        The numpy array z with the logistic function applied element-wise\n",
    "\n",
    "    Hints\n",
    "     - You should be using numpy and array broadcasting here \n",
    "     - np.exp may be helpful \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6537e44",
   "metadata": {},
   "source": [
    "Once you are done with your implementation, try visualizing it to\n",
    "double-check the implementation. Does the graph below look reasonable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef859543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a numpy array containing evenly separated numbers, from -10 to 10 \n",
    "# (inclusive)\n",
    "z = np.arange(-10, 10, .01)\n",
    "\n",
    "# Call our sigmoid function on the newly created array\n",
    "output = logistic_function(z)\n",
    "\n",
    "# Plot\n",
    "plt.plot(z, output, color='blue', lw=2)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('logistic_function(z)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4705962",
   "metadata": {},
   "source": [
    "### 2B. Negative log likelihood (NLL) loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11644d",
   "metadata": {},
   "source": [
    "Another building block of binary `LogisticRegression` is the `Negative Log Likelihood` loss function. Implement this loss function (as discussed in lecture) below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLLLoss(pred_prob: np.ndarray, y_true: np.ndarray, epsilon=1e-20) -> float:\n",
    "    \"\"\"\n",
    "    \n",
    "    Args: \n",
    "        - pred_prob: A 1-D vector of your model's predicted probability of y=1\n",
    "        - y_true: A 1-D vector of the true class labels \n",
    "        - epsilon (optional): a very small number added to the input every time np.log is called\n",
    "            for numerical stability \n",
    "        \n",
    "    Returns: \n",
    "        - float: The negative log likelihood\n",
    "        \n",
    "    Hints:\n",
    "    - If you run into \"RuntimeWarning: divide by zero encountered in log\" \n",
    "        issues, try adding a very small number epsilon (like 1e-10) to the \n",
    "        input any time you call np.log(). This will ensure that the input to \n",
    "        the log is never exactly 0, which gives an undefined result.\n",
    "    - Recall from lecture, your solution should average over the number of examples \n",
    "    \"\"\"\n",
    "    assert pred_prob.shape == y_true.shape #inputs must be the same size \n",
    "    \n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5160bf58",
   "metadata": {},
   "source": [
    "The loss is supposed to represent how far our prediction is from the true label. In other words, if our prediction is very far from the truth, the loss should be very high. If our prediction gets closer to the true value, the loss should drop towards 0. Let's consider a few cases to understand how the loss should change on specific input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da451cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_loss_single_example(pred_prob_single, y_true_single):\n",
    "    print(\"Predicted Prob = {}, True = {} : Loss = {}\".format(\n",
    "          pred_prob_single, y_true_single, NLLLoss(np.array([pred_prob_single]),\n",
    "                                        np.array([y_true_single]))))\n",
    "print_loss_single_example(0.0, 0)\n",
    "print_loss_single_example(0.0, 1)\n",
    "print_loss_single_example(0.1, 1)\n",
    "print_loss_single_example(0.3, 1)\n",
    "print_loss_single_example(0.5, 1)\n",
    "print_loss_single_example(0.7, 1)\n",
    "print_loss_single_example(0.9, 1)\n",
    "print_loss_single_example(0.99, 1)\n",
    "print_loss_single_example(0.999999, 1)\n",
    "print_loss_single_example(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your impelmentation works for vectors as well \n",
    "pred_prob = np.array([0.8, 0.2, 0.7])\n",
    "y_true = np.array([1, 0, 1])\n",
    "NLLLoss(pred_prob, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d3db3",
   "metadata": {},
   "source": [
    "### 2C. Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433eb3ad",
   "metadata": {},
   "source": [
    "During lecture, we derived the gradient of the negative log likelihood. Implement that gradient in the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_nll(X: np.ndarray, \n",
    "                 y: np.ndarray, \n",
    "                 theta: np.ndarray) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Returns the gradient of the negative log likelihood (NLL) \n",
    "    with respect to the inputs: \n",
    "        - X: shape=(num_examples, num_words)\n",
    "        - y: shape=(num_examples,)\n",
    "        - theta: shape=(num_words,)\n",
    "        \n",
    "    Hints: \n",
    "        - At some point during this function, you should call logistic_function \n",
    "            which you implemented above\n",
    "        - You should not have any for-loops in this code. You should use \n",
    "            numpy array operations and broadcasting. Otherwise your implementation\n",
    "            will run significantly slower. \n",
    "        - Section 5.6.4 from the J&M textbook will be helpful here \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e879b",
   "metadata": {},
   "source": [
    "We give you a very small toy dataset in the cell below. We encourage you to calculate the gradient by hand and then check if your implementation matches what you get on paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 1], \n",
    "              [0, 1, 1]])\n",
    "y = np.array([1, 0])\n",
    "theta = np.array([-1, 0, 1])\n",
    "gradient = gradient_nll(X, y, theta)\n",
    "print('Gradient =', gradient)\n",
    "assert gradient.shape[0] == theta.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fa899",
   "metadata": {},
   "source": [
    "Now that you have written the function for the gradient, we have implemented mini-batch stochastic gradient descent for you below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "def stochastic_gradient_descent(X: np.ndarray,\n",
    "                     Y: np.ndarray,\n",
    "                     batch_size: int = 2000,\n",
    "                     alpha: float = 0.5,\n",
    "                     num_iterations: int = 1000,\n",
    "                     print_every: int = 100,\n",
    "                     early_stopping_value: float = 1e-8, \n",
    "                     l2_regularization_strength: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Runs mini-batch stochasitc gradient descent on the provided data and returns the resulting\n",
    "    trained weight vector. \n",
    "\n",
    "    Args:\n",
    "        - X: A numpy array of shape (num_examples, num_words) containing\n",
    "           the training data.\n",
    "        - Y: A numpy array of shape (num_examples,) containing the training\n",
    "            labels.\n",
    "        - batch_size: The number of examples in each batch.\n",
    "        - alpha: The learning rate for gradient descent.\n",
    "        - num_iterations: The number of iterations to run gradient descent for.\n",
    "        - print_every: How often (after how many iterations) to print the\n",
    "                    loss and iteration number.\n",
    "        - early_stopping_value: The early stopping condition. When the absolute change\n",
    "                 in the loss is less than early_stopping_value, gradient descent will\n",
    "                 stop early.\n",
    "        - l2_regularization_strength: the constant in front of the L2 penalty term. Set to 0 \n",
    "            if you would like to turn off regularization. \n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The learned weight vector theta \n",
    "        \n",
    "    Notes: \n",
    "        - Remember we are \"absorbing the bias\" like we discussed during lecture \n",
    "    \"\"\"\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    \n",
    "    # add ones to X to \"absorb the bias\"\n",
    "    stack = np.ones((X.shape[0], 1))\n",
    "    X = np.concatenate((X, stack), axis=1)\n",
    "    \n",
    "    # initalize the weight vector \n",
    "    theta = np.zeros((X.shape[1],))\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(num_iterations):\n",
    "        #create batches \n",
    "        if batch_size >= X.shape[0]: \n",
    "            X_batch = X\n",
    "            Y_batch = Y\n",
    "        else: #randomly choose batch_size number of examples \n",
    "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
    "            X_batch = X[batch_indices]\n",
    "            Y_batch = Y[batch_indices]\n",
    "        \n",
    "        # One step through gradient descent \n",
    "        Y_batch_pred_prob = logistic_function(np.dot(X_batch, theta))\n",
    "        d_theta = gradient_nll(X_batch, Y_batch, theta)\n",
    "        # Make sure to account for L2 regularization in the gradient update equation\n",
    "        theta -= (alpha * d_theta +2*l2_regularization_strength*theta)\n",
    "        prev_loss = loss \n",
    "        loss = NLLLoss(Y_batch_pred_prob, Y_batch)\n",
    "        # Add L2 regularization to the loss \n",
    "        loss += l2_regularization_strength * np.sum(np.square(theta))\n",
    "        \n",
    "        # early stopping condition \n",
    "        if abs(prev_loss - loss) < early_stopping_value:\n",
    "            break\n",
    "            \n",
    "        #print some diagnostics \n",
    "        if (i+1) % print_every == 0:\n",
    "            Y_batch_pred = np.zeros(Y_batch.shape)\n",
    "            Y_batch_pred[Y_batch_pred_prob >= 0.5] = 1\n",
    "            accuracy = np.mean(Y_batch_pred == Y_batch)\n",
    "            print(f\"Iteration {i + 1}/{num_iterations}: Batch Accuracy: {np.round(accuracy,2)},  Batch Loss = {np.round(loss,3)}\")\n",
    "    return theta "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a884057",
   "metadata": {},
   "source": [
    "### 2D. LogisticRegressionClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c729ff",
   "metadata": {},
   "source": [
    "Complete the implementation of the `LogisticRegessionClassifier` below.\n",
    "\n",
    "You are welcome to create additional helper functions *within* the class if your implementation requires it. However, any functions you write outside of the `LogisticRegessionClassifier` class cannot be accessed by the autograder and may cause it to fail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af2c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the implementation below \n",
    "class LogisticRegressionClassifier:\n",
    "    \"\"\"\n",
    "    Implements Naive Bayes Classifier \n",
    "    Includes Laplace smoothing (add-1 smoothing) during training \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 batch_size: int = 2000,\n",
    "                 alpha: float = 0.5,\n",
    "                 num_iterations: int = 1000,\n",
    "                 print_every: int = 100,\n",
    "                 early_stopping_value: float = 1e-8,\n",
    "                 l2_regularization_strength: float = 0.01):\n",
    "        \n",
    "        # Parameters used by stochastic gradient descent \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.num_iterations = num_iterations\n",
    "        self.print_every = print_every\n",
    "        self.early_stopping_value = early_stopping_value\n",
    "        self.l2_regularization_strength = l2_regularization_strength\n",
    "        \n",
    "        # TODO: add other data structures needed for predict() or train()\n",
    "        # CODE START\n",
    "        pass \n",
    "        # CODE END\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train:np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        This method inputs the training data (X, y) both of which are \n",
    "        numpy arrays and trains the Logistic Regression classifier.  \n",
    "        \n",
    "        Hints: \n",
    "            - You should call stochastic_gradient_descent() at some\n",
    "                point in this function \n",
    "            - You should pass in the parameters we initialize above in __init__ that are used by \n",
    "                stochastic gradient descent \n",
    "        \"\"\"\n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END\n",
    "             \n",
    "    def predict(self, X_test: np.ndarray) -> Union[List[int], List[float]]:\n",
    "        \"\"\"\n",
    "        This method inputs the examples X_test (a numpy array)\n",
    "        \n",
    "        This method returns a list of int variables that are the predicted\n",
    "        labels (e.g. 0 or 1)\n",
    "        \n",
    "        You should use a decision threshold of P(y=1|x) > 0.5 to predict 1 as the label for y.\n",
    "            \n",
    "        Hints: \n",
    "            - At some point in this method, you should use the theta vector \n",
    "            you computed earlier in train()\n",
    "            - Remember that during stochastic_gradient_descent() we \"absorb the bias\" for theta.\n",
    "                How do you modify X_test as a result? \n",
    "        \"\"\"\n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END\n",
    "\n",
    "    def get_weights(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        TODO: Implement a method to return the trained weights.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Trained weights\n",
    "            \n",
    "        Hint: \n",
    "            - Don't overthink this. This method should be one line of code if you set-up \n",
    "            your class correclty \n",
    "        \"\"\"\n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5045de2",
   "metadata": {},
   "source": [
    "#### Debugging on \"toy\" corpus \n",
    "\n",
    "Like most real-world NLP systems, it can be helpful to examine the correctness of our code on a small \"toy\" dataset that we can analytically calculate the answers for.\n",
    "\n",
    "Below, we implemented the example from lecture with toy corpus words \"repair, \"beyond\" and \"good\". Use this to check your implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify this cell\n",
    "toy_X_train = np.array([[1, 0, 0], \n",
    "                        [0, 1, 1], \n",
    "                        [1, 1, 0], \n",
    "                        [0, 0, 1]])\n",
    "toy_y_train = np.array([0, 1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaec397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call to check your implementation \n",
    "lr_classifier_toy = LogisticRegressionClassifier(batch_size=4, num_iterations=100, \n",
    "                                                 print_every=10, l2_regularization_strength=0)\n",
    "lr_classifier_toy.train(toy_X_train, toy_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5e00f",
   "metadata": {},
   "source": [
    "With 100 epochs, you should be seeing `Batch Loss` values of less than `0.05` in the cell above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adc51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr_classifier_toy.predict(toy_X_train)\n",
    "print('Models predictions for toy_X_train = ', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652d124",
   "metadata": {},
   "source": [
    "If you trained your model correctly, you should be getting an accuracy of 100\\% for this toy corpus training data. Knowing this, how can you check that `y_pred` in the cell above is giving you the correct answer? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f0fa2",
   "metadata": {},
   "source": [
    "## 3. Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc9aa94",
   "metadata": {},
   "source": [
    "### 3A. Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c407c9",
   "metadata": {},
   "source": [
    "We will evaluate your classifier on the `triage` dataset, the same one that we evaluated on Naive Bayes in HW3. This will give you an opportunity to compare the performance of the two classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have run the cells in Part 1B that load the triage data \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL \n",
    "lr_classifier = LogisticRegressionClassifier(batch_size=2000, num_iterations=1000, print_every=100, alpha=1, l2_regularization_strength=0.0005)\n",
    "lr_classifier.train(data['X_train'], data['Y_train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e8b68",
   "metadata": {},
   "source": [
    "In our reference implementation, the cell above takes about 2 seconds to run. If yours is taking longer than several minutes, you may want to return to the functions you wrote earlier and make sure they are vectorized and using `numpy` effectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43918497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS CELL \n",
    "Y_pred = lr_classifier.predict(data['X_dev'])\n",
    "accuracy = np.mean(Y_pred == data['Y_dev']) \n",
    "print('Development accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61c987",
   "metadata": {},
   "source": [
    "### 3B. Inspecting word weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45ee58",
   "metadata": {},
   "source": [
    "For Logistic Regression, we are able to inspect the individual weights, $\\theta_k$, learned for each word in the vocabulary. Let's inspect these to see what our model learned for the `triage` data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41257510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "\n",
    "words = data['x_columns_as_words']\n",
    "weights = lr_classifier.get_weights()\n",
    "\n",
    "words_to_weights = [(words[i], weights[i])\n",
    "                    for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "\n",
    "print('Highest weights on words')\n",
    "print('==='*15)\n",
    "top_10_words = sorted(words_to_weights,\n",
    "                   key=operator.itemgetter(1), reverse=True)[:10]\n",
    "for word, weight in top_10_words:\n",
    "    print(\"{0:<15} weight = {1:.2f}\".format(word, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO MODIFY THIS CELL \n",
    "\n",
    "print('Lowest weights on words')\n",
    "print('==='*15)\n",
    "\n",
    "bottom_10_words = sorted(words_to_weights,\n",
    "                            key=operator.itemgetter(1))[:10]\n",
    "for word, weight in bottom_10_words:\n",
    "    print(\"{0:<15} weight = {1:.2f}\".format(word, weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead4ba6",
   "metadata": {},
   "source": [
    "### 3C. Tradeoffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261a4d1",
   "metadata": {},
   "source": [
    "You have now implemented both Logistic Regression and Naive Bayes and evaluated the trained classifiers on the same datsaet. What do you see as the tradeoffs of these two models' assumptions, ease of implementation, interpretability, and predictive performance?\n",
    "\n",
    "(We are expecting at minimum *three* complete sentences for full credit. There may be more than one correct answer for this question.) \n",
    "\n",
    "*Hints*: \n",
    "- Compare the interprebability and validity of the \"highest weights on words\" above for Logistic Regression to the p(label|word) you learned for Naive Bayes.\n",
    "- What aspects of the Naive Bayes implementation last week versus the Logistic Regression implementation this week were easier? More reliable? \n",
    "- Discuss the predictive performance (accuracies) of the two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c22c4",
   "metadata": {},
   "source": [
    "**Part 3C Answer:**\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2c605",
   "metadata": {},
   "source": [
    "## (Optional) 4. Extra Credit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8115e8d8",
   "metadata": {},
   "source": [
    "*Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847143c0",
   "metadata": {},
   "source": [
    "Above, we have implemented the most basic version of Logistic Regression. Try to extend it and/or make it better. Some suggestions to try:\n",
    "- Extend for multi-class (multinomial) logistic regression (J&M Ch. 5.3)\n",
    "- Extend your classifier to also output the predicted probabilities. Then evaluate your classifier on calibration. See the calibration section of [Katie's tutorial for social scientists](https://colab.research.google.com/drive/1ulQSwlSlWTEglzBGVQXKstueIaX5Gm1f?usp=sharing). \n",
    "- In stochastic gradient descent, instead of using a single learning rate, use an adaptative learning rate, e.g. [Adam](https://arxiv.org/abs/1412.6980). \n",
    "- Implement L1 regularization (J&M Ch. 5.7)\n",
    "- Change the decision threshold from 0.5 to something else and evaluate how this changes your precision and recall.\n",
    "- Use a grid search and cross validation to tune all hyperparameters. \n",
    "- Anything else you want to try! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa70a7",
   "metadata": {},
   "source": [
    "For an extra credit submission please create a separate `.ipynb` file and submit to `Extra Credit: HW 3` submission on Gradescope. \n",
    "\n",
    "This is to ensure your extra credit work does not interfere with or inhibit your main submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abde531",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70deea1",
   "metadata": {},
   "source": [
    "**Processing reporting.** Please record your answers to the questions below by writing directly in this Markdown cell.\n",
    "\n",
    "If you talked with any of your classmates on this assignment please list their names here:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*\n",
    "\n",
    "Approximately how much time did you spend on this assignment:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ff36d",
   "metadata": {},
   "source": [
    "**Download zip.** Once you're satsified with your solution, save this file and run the cell below to automatically zip your file. This will produce `submission.zip` in the same folder as this file (same folder as `hw3.ipynb`). \n",
    "\n",
    "Submit `submission.zip` to Gradescope. \n",
    "\n",
    "*Note:* This script assumes that you have the `zip` utility installed and you can use `bash` on your system. If the cell below does not work you may need to zip your file manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbe590",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw3.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw3.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f074d161a9daf0c140078b93d71c6780d1f24507d9304caedb73ac00a5824ebf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
